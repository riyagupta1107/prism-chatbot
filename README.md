
# Prism Chatbot ğŸ¤–âœ¨

A smart chatbot system built using a **Retrieval-Augmented Generation (RAG) model** with **Pinecone** as the vector database.
It is designed to answer queries related to items in a knowledge base with **fast, accurate, and context-aware responses**.

This repository also contains an **experimental alternate approach** (Work In Progress) where we explored building a custom LLM for model-related queries.

---

## ğŸš€ Features

* ğŸ” **RAG-based retrieval** â€“ fetches relevant context before generating responses.
* ğŸ“¦ **Pinecone integration** â€“ efficient, scalable vector search for item-related queries.
* ğŸ’¬ **Conversational interface** â€“ interact with the bot just like a human.
* ğŸ§ª **Experimental LLM attempt** â€“ an alternate design being explored (in progress).

---

## ğŸ› ï¸ Tech Stack

* **Language Model**: OpenAI / compatible LLM API
* **Vector Database**: [Pinecone](https://www.pinecone.io/)
* **Backend**: Python
* **Frameworks**: LangChain (for RAG pipeline), FastAPI/Flask (if applicable)
* **Environment**: Conda / Virtualenv

---

## ğŸ“‚ Folder Structure

```
prism-chatbot/
â”‚
â”œâ”€â”€ main/                 # âœ… Working chatbot (RAG + Pinecone)
â”‚   â”œâ”€â”€ app.py            # Entry point for chatbot
â”‚   â”œâ”€â”€ rag_pipeline.py   # Retrieval-Augmented Generation pipeline
â”‚   â”œâ”€â”€ pinecone_utils.py # Pinecone database integration
â”‚   â””â”€â”€ requirements.txt  # Dependencies
â”‚
â”œâ”€â”€ alternate/            # ğŸ§ª Experimental custom LLM approach (incomplete)
â”‚   â””â”€â”€ model_attempt.py  # Early work on a model-specific Q&A bot
â”‚
â””â”€â”€ README.md             # ğŸ“– You are here
```

---

## âš¡ Getting Started

### 1. Clone the repo

```bash
git clone https://github.com/riyagupta1107/prism-chatbot.git
cd prism-chatbot/main
```

### 2. Install dependencies

```bash
pip install -r requirements.txt
```

### 3. Set up environment variables

Create a `.env` file with your API keys:

```
OPENAI_API_KEY=your_api_key
PINECONE_API_KEY=your_pinecone_key
PINECONE_ENV=your_pinecone_environment
```

### 4. Run the chatbot

```bash
python app.py
```

---

## ğŸ¯ Usage

Once running, you can ask the chatbot **item-related queries**, and it will:

1. Retrieve relevant context from Pinecone
2. Use the LLM to generate a contextual answer
3. Reply in natural language

---
## ğŸ§ª Work in Progress: Alternate LLM Approach

In addition to the production-ready **RAG + Pinecone chatbot**, this repository also contains an **experimental project** under the `alternate/` folder.

### ğŸŒŸ The Vision

The idea behind this alternate approach is to **move beyond retrieval-based systems** and explore whether a **custom domain-focused LLM** could directly answer queries. Instead of always depending on Pinecone for context, this model aims to:

* Handle **model-related technical queries** more natively.
* Allow **fine-tuning and domain specialization**.
* Explore the trade-offs between **retrieval-based vs. generative-only** solutions.

This reflects a forward-looking exploration: â€œWhat if the chatbot could learn and answer intelligently on its own, without needing external lookups every time?â€

---

### ğŸ› ï¸ Current Progress (in `alternate/`)

* **`model_attempt.py`** â†’ Early prototype script experimenting with generating responses directly from an LLM.
* **Key Concepts Tried**:

  * Prompt engineering for model-specific Q&A.
  * Structuring queries without Pinecone dependency.
  * Testing response quality with smaller datasets.

ğŸ‘‰ While this path is still under construction, it highlights the teamâ€™s **curiosity and commitment to innovation**.
The progress so far provides a solid foundation to build on â€” and the lessons learned here will feed back into improving the main chatbot.

---
## ğŸŒ± Future Scope

* Enhance **alternate LLM project** for domain-specific Q&A.
* Add **frontend UI** for better user interaction.
* Support **multi-database backends** beyond Pinecone.
* **Hybrid design** â†’ Combine this LLM with lightweight embeddings for fallback retrieval.
* **Fine-tuning experiments** â†’ Train a smaller domain-specific model for faster, focused answers.
* **Memory integration** â†’ Enable multi-turn conversations with persistent context.
* **Benchmarking** â†’ Compare this approach against the RAG system to measure accuracy vs. flexibility.

---




