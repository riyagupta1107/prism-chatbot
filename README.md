
# Prism Chatbot ğŸ¤–âœ¨

A smart chatbot system built using a **Retrieval-Augmented Generation (RAG) model** with **Pinecone** as the vector database.
It is designed to answer queries related to items in a knowledge base with **fast, accurate, and context-aware responses**.

This repository also contains an **experimental alternate approach** (WIP) where we explored building a custom LLM for model-related queries.

---

## ğŸš€ Features

* ğŸ” **RAG-based retrieval** â€“ fetches relevant context before generating responses.
* ğŸ“¦ **Pinecone integration** â€“ efficient, scalable vector search for item-related queries.
* ğŸ’¬ **Conversational interface** â€“ interact with the bot just like a human.
* ğŸ§ª **Experimental LLM attempt** â€“ an alternate design being explored (in progress).

---

## ğŸ› ï¸ Tech Stack

* **Language Model**: OpenAI / compatible LLM API
* **Vector Database**: [Pinecone](https://www.pinecone.io/)
* **Backend**: Python
* **Frameworks**: LangChain (for RAG pipeline), FastAPI/Flask (if applicable)
* **Environment**: Conda / Virtualenv

---

## ğŸ“‚ Folder Structure

```
prism-chatbot/
â”‚
â”œâ”€â”€ main/                 # âœ… Working chatbot (RAG + Pinecone)
â”‚   â”œâ”€â”€ app.py            # Entry point for chatbot
â”‚   â”œâ”€â”€ rag_pipeline.py   # Retrieval-Augmented Generation pipeline
â”‚   â”œâ”€â”€ pinecone_utils.py # Pinecone database integration
â”‚   â””â”€â”€ requirements.txt  # Dependencies
â”‚
â”œâ”€â”€ alternate/            # ğŸ§ª Experimental custom LLM approach (incomplete)
â”‚   â”œâ”€â”€ model_attempt.py  # Early work on a model-specific Q&A bot
â”‚   â””â”€â”€ notes.md          # Development notes / ideas
â”‚
â””â”€â”€ README.md             # ğŸ“– You are here
```

---

## âš¡ Getting Started

### 1. Clone the repo

```bash
git clone https://github.com/riyagupta1107/prism-chatbot.git
cd prism-chatbot/main
```

### 2. Install dependencies

```bash
pip install -r requirements.txt
```

### 3. Set up environment variables

Create a `.env` file with your API keys:

```
OPENAI_API_KEY=your_api_key
PINECONE_API_KEY=your_pinecone_key
PINECONE_ENV=your_pinecone_environment
```

### 4. Run the chatbot

```bash
python app.py
```

---

## ğŸ¯ Usage

Once running, you can ask the chatbot **item-related queries**, and it will:

1. Retrieve relevant context from Pinecone
2. Use the LLM to generate a contextual answer
3. Reply in natural language

---

## ğŸŒ± Future Scope

* Enhance **alternate LLM project** for domain-specific Q&A.
* Add **frontend UI** for better user interaction.
* Explore **fine-tuning** for improved domain accuracy.
* Support **multi-database backends** beyond Pinecone.

---


Would you like me to also add **badges (Python version, License, etc.) and a sample demo interaction block** so the README looks even more professional for GitHub?
